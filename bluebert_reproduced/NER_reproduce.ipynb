{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NER.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNQOWxo6Rg1SYa0TFTod2Gp"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DUlh04VWGqzG","executionInfo":{"status":"ok","timestamp":1614607872191,"user_tz":360,"elapsed":330169,"user":{"displayName":"Bing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjpjJjrneW_zAZuv7MPsrzui3kXSu4kQScGBydW=s64","userId":"15613447837470351648"}},"outputId":"60b2106f-4f1a-45a2-8857-c24025189205"},"source":["!pip install tensorflow==1.15.4\r\n","!pip install google-api-python-client\r\n","!pip install oauth2client\r\n","!pip install tqdm\r\n","!pip install pandas==0.25.3\r\n","!pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html\r\n","!pip install allennlp"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.15.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/9f/18e031f52c46b4c63f6a6f064cd2dd85a2f730ca5dc44ef2f25375990dce/tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5MB)\n","\u001b[K     |████████████████████████████████| 110.5MB 87kB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (0.2.0)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (3.12.4)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (1.32.0)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 43.6MB/s \n","\u001b[?25hCollecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 45.4MB/s \n","\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n","\u001b[K     |████████████████████████████████| 20.1MB 63.1MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (0.36.2)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (0.10.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (1.15.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (1.12.1)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (0.8.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (3.3.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4) (1.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.4) (53.0.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (3.3.4)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4) (2.10.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (3.7.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (3.7.4.3)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=a3338c92bacb63a8095027a606ce4e5333e6d2c635f6b0c9bfd45b08612f3a99\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: gast, numpy, tensorboard, tensorflow-estimator, keras-applications, tensorflow\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Found existing installation: tensorboard 2.4.1\n","    Uninstalling tensorboard-2.4.1:\n","      Successfully uninstalled tensorboard-2.4.1\n","  Found existing installation: tensorflow-estimator 2.4.0\n","    Uninstalling tensorflow-estimator-2.4.0:\n","      Successfully uninstalled tensorflow-estimator-2.4.0\n","  Found existing installation: tensorflow 2.4.1\n","    Uninstalling tensorflow-2.4.1:\n","      Successfully uninstalled tensorflow-2.4.1\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 numpy-1.18.5 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (1.7.12)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (3.0.1)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (0.0.4)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (1.15.0)\n","Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (0.17.4)\n","Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client) (1.27.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (4.2.1)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (53.0.0)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (4.7.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client) (0.4.8)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (4.1.3)\n","Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client) (0.17.4)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client) (0.4.8)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client) (0.2.8)\n","Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client) (1.15.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n","Collecting pandas==0.25.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/e0/a1b39cdcb2c391f087a1538bc8a6d62a82d0439693192aef541d7b123769/pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n","\u001b[K     |████████████████████████████████| 10.4MB 5.9MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3) (2.8.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3) (1.18.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.6.1->pandas==0.25.3) (1.15.0)\n","\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n","Installing collected packages: pandas\n","  Found existing installation: pandas 1.1.5\n","    Uninstalling pandas-1.1.5:\n","      Successfully uninstalled pandas-1.1.5\n","Successfully installed pandas-0.25.3\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pandas"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch===1.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4MB)\n","\u001b[K     |████████████████████████████████| 753.4MB 22kB/s \n","\u001b[?25hCollecting torchvision===0.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/32/cb0e4c43cd717da50258887b088471568990b5a749784c465a8a1962e021/torchvision-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0MB)\n","\u001b[K     |████████████████████████████████| 4.0MB 45.3MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision===0.5.0) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision===0.5.0) (1.18.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision===0.5.0) (7.0.0)\n","Installing collected packages: torch, torchvision\n","  Found existing installation: torch 1.7.1+cu101\n","    Uninstalling torch-1.7.1+cu101:\n","      Successfully uninstalled torch-1.7.1+cu101\n","  Found existing installation: torchvision 0.8.2+cu101\n","    Uninstalling torchvision-0.8.2+cu101:\n","      Successfully uninstalled torchvision-0.8.2+cu101\n","Successfully installed torch-1.4.0 torchvision-0.5.0\n","Collecting allennlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/bd/c75fa01e3deb9322b637fe0be45164b40d43747661aca9195b5fb334947c/allennlp-2.1.0-py3-none-any.whl (585kB)\n","\u001b[K     |████████████████████████████████| 593kB 8.8MB/s \n","\u001b[?25hCollecting tensorboardX>=1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\u001b[K     |████████████████████████████████| 317kB 18.1MB/s \n","\u001b[?25hRequirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.12)\n","Requirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n","Collecting jsonpickle\n","  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n","Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n","Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n","\u001b[K     |████████████████████████████████| 266kB 13.1MB/s \n","\u001b[?25hCollecting torchvision<0.9.0,>=0.8.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/df/969e69a94cff1c8911acb0688117f95e1915becc1e01c73e7960a2c76ec8/torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8MB)\n","\u001b[K     |████████████████████████████████| 12.8MB 8.9MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.41.1)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 37.0MB/s \n","\u001b[?25hCollecting overrides==3.1.0\n","  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.7.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n","Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n","Collecting torch<1.8.0,>=1.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n","\u001b[K     |████████████████████████████████| 776.8MB 19kB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.10.0)\n","Collecting transformers<4.4,>=4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 39.6MB/s \n","\u001b[?25hCollecting boto3<2.0,>=1.14\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/c8/b5aac643697038ef6eb8c11c73b9ee9c2dc8cb2bc95cda2d4ee656167644/boto3-1.17.17-py2.py3-none-any.whl (130kB)\n","\u001b[K     |████████████████████████████████| 133kB 52.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.18.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp) (1.15.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp) (3.12.4)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (3.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.1.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (2.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (7.4.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.8.2)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (53.0.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle->allennlp) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2020.12.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.9.0,>=0.8.1->allennlp) (7.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp) (1.0.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.8.0,>=1.6.0->allennlp) (3.7.4.3)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (20.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.4,>=4.1->allennlp) (20.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.4,>=4.1->allennlp) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 46.2MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 31.8MB/s \n","\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting botocore<1.21.0,>=1.20.17\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/fb/7ea265e28306dde068c74e6792affd4df43e51784384829c69142042ad56/botocore-1.20.17-py2.py3-none-any.whl (7.3MB)\n","\u001b[K     |████████████████████████████████| 7.3MB 39.2MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp) (3.4.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.4,>=4.1->allennlp) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.4,>=4.1->allennlp) (7.1.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.17->boto3<2.0,>=1.14->allennlp) (2.8.1)\n","Building wheels for collected packages: jsonnet, overrides, sacremoses\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388797 sha256=ba28aca69739b04f5aca7a32070dafddee17aaecc50fff567803fada05c10ab7\n","  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=abfaf8f1ab33249509edab69ae6b3376c043166b5c8380ffb3a85e88c3ca65be\n","  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=76a21e6e4009666a6b018291a2564e8eee0ade4dbb64aad12d9d6e514e656801\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built jsonnet overrides sacremoses\n","\u001b[31mERROR: botocore 1.20.17 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: tensorboardX, jsonpickle, jsonnet, torch, torchvision, sentencepiece, overrides, sacremoses, tokenizers, transformers, jmespath, botocore, s3transfer, boto3, allennlp\n","  Found existing installation: torch 1.4.0\n","    Uninstalling torch-1.4.0:\n","      Successfully uninstalled torch-1.4.0\n","  Found existing installation: torchvision 0.5.0\n","    Uninstalling torchvision-0.5.0:\n","      Successfully uninstalled torchvision-0.5.0\n","Successfully installed allennlp-2.1.0 boto3-1.17.17 botocore-1.20.17 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-2.0.0 overrides-3.1.0 s3transfer-0.3.4 sacremoses-0.0.43 sentencepiece-0.1.95 tensorboardX-2.1 tokenizers-0.10.1 torch-1.7.1 torchvision-0.8.2 transformers-4.3.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-rghN-RHG9QI","executionInfo":{"status":"ok","timestamp":1614610784968,"user_tz":360,"elapsed":3242787,"user":{"displayName":"Bing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjpjJjrneW_zAZuv7MPsrzui3kXSu4kQScGBydW=s64","userId":"15613447837470351648"}},"outputId":"e9f4bcf3-e12e-45f7-d494-a6ce2973eb2b"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eEaBxRvkG-eG","executionInfo":{"status":"ok","timestamp":1614610788147,"user_tz":360,"elapsed":506,"user":{"displayName":"Bing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjpjJjrneW_zAZuv7MPsrzui3kXSu4kQScGBydW=s64","userId":"15613447837470351648"}}},"source":["import os\r\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/Capstone/bluebert/')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TeHr2A_xHHFJ","executionInfo":{"status":"ok","timestamp":1614610789042,"user_tz":360,"elapsed":411,"user":{"displayName":"Bing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjpjJjrneW_zAZuv7MPsrzui3kXSu4kQScGBydW=s64","userId":"15613447837470351648"}},"outputId":"0b4cd961-710c-41c9-868d-7bb1777cf6bd"},"source":["! echo $PYTHONPATH\r\n","%env PYTHONPATH=\"$/env/python:/content/drive/MyDrive/Colab Notebooks/Capstone/bluebert/\"\r\n","! echo $PYTHONPATH"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/env/python\n","env: PYTHONPATH=\"$/env/python:/content/drive/MyDrive/Colab Notebooks/Capstone/bluebert/\"\n","\"$/env/python:/content/drive/MyDrive/Colab Notebooks/Capstone/bluebert/\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8gO6bCLaHLOC","executionInfo":{"status":"ok","timestamp":1614610796275,"user_tz":360,"elapsed":6433,"user":{"displayName":"Bing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjpjJjrneW_zAZuv7MPsrzui3kXSu4kQScGBydW=s64","userId":"15613447837470351648"}},"outputId":"e25fc981-188d-4b3f-f251-4ff1ecff3613"},"source":["from __future__ import absolute_import\r\n","from __future__ import division\r\n","from __future__ import print_function\r\n","\r\n","import csv\r\n","import os\r\n","\r\n","\r\n","print(os.getcwd())\r\n","import bert\r\n","\r\n","from bert import modeling\r\n","from bert import optimization\r\n","from bert import tokenization\r\n","import tensorflow as tf\r\n","import scipy"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/Capstone/bluebert\n","WARNING:tensorflow:From /content/drive/MyDrive/Colab Notebooks/Capstone/bluebert/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"XhrUIKfCTXWX","executionInfo":{"status":"error","timestamp":1614612963847,"user_tz":360,"elapsed":7528,"user":{"displayName":"Bing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjpjJjrneW_zAZuv7MPsrzui3kXSu4kQScGBydW=s64","userId":"15613447837470351648"}},"outputId":"e2454dfe-756b-4dfb-b98b-bdc7143775be"},"source":["flags = tf.flags\r\n","\r\n","FLAGS = flags.FLAGS\r\n","\r\n","flags.DEFINE_string(\r\n","    \"task_name\", \"bc5cdr\", \"The name of the task to train.\"\r\n",")\r\n","\r\n","flags.DEFINE_string(\r\n","    \"data_dir\", \"datasets/BC5CDR/disease/\",\r\n","    \"The input datadir.\",\r\n",")\r\n","\r\n","flags.DEFINE_string(\r\n","    \"output_dir\", \"output/BC5CDRdiseas_10/\",\r\n","    \"The output directory where the model checkpoints will be written.\"\r\n",")\r\n","\r\n","flags.DEFINE_string(\r\n","    \"bert_config_file\", \"bluebert_model/bert_config.json\",\r\n","    \"The config json file corresponding to the pre-trained BERT model.\"\r\n",")\r\n","\r\n","flags.DEFINE_string(\r\n","    \"vocab_file\",\"bluebert_model/vocab.txt\", \r\n","    \"The vocabulary file that the BERT model was trained on.\")\r\n","\r\n","flags.DEFINE_string(\r\n","    \"init_checkpoint\", \"bluebert_model/bert_model.ckpt\",\r\n","    \"Initial checkpoint (usually from a pre-trained BERT model).\"\r\n",")\r\n","\r\n","flags.DEFINE_bool(\r\n","    \"do_lower_case\", True,\r\n","    \"Whether to lower case the input text.\"\r\n",")\r\n","\r\n","flags.DEFINE_integer(\r\n","    \"max_seq_length\", 128,\r\n","    \"The maximum total input sequence length after WordPiece tokenization.\"\r\n",")\r\n","\r\n","flags.DEFINE_bool(\r\n","    \"do_train\", True,\r\n","    \"Whether to run training.\"\r\n",")\r\n","flags.DEFINE_bool(\r\n","    \"use_tpu\", False,\r\n","    \"Whether to use TPU or GPU/CPU.\")\r\n","\r\n","flags.DEFINE_bool(\r\n","    \"do_eval\", True,\r\n","    \"Whether to run eval on the dev set.\")\r\n","\r\n","flags.DEFINE_bool(\r\n","    \"do_predict\", True,\r\n","    \"Whether to run the model in inference mode on the test set.\")\r\n","\r\n","flags.DEFINE_integer(\r\n","    \"train_batch_size\", 32,\r\n","    \"Total batch size for training.\")\r\n","\r\n","flags.DEFINE_integer(\r\n","    \"eval_batch_size\", 8,\r\n","    \"Total batch size for eval.\")\r\n","\r\n","flags.DEFINE_integer(\r\n","    \"predict_batch_size\", 8,\r\n","    \"Total batch size for predict.\")\r\n","\r\n","flags.DEFINE_float(\r\n","    \"learning_rate\", 5e-5,\r\n","    \"The initial learning rate for Adam.\")\r\n","\r\n","flags.DEFINE_float(\r\n","    \"num_train_epochs\", 10.0,\r\n","    \"Total number of training epochs to perform.\")\r\n","\r\n","flags.DEFINE_float(\r\n","    \"warmup_proportion\", 0.1,\r\n","    \"Proportion of training to perform linear learning rate warmup for. \"\r\n","    \"E.g., 0.1 = 10% of training.\")\r\n","\r\n","flags.DEFINE_integer(\r\n","    \"save_checkpoints_steps\", 1000,\r\n","    \"How often to save the model checkpoint.\")\r\n","\r\n","flags.DEFINE_integer(\r\n","    \"iterations_per_loop\", 1000,\r\n","    \"How many steps to make in each estimator call.\")\r\n","\r\n","tf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\r\n","\r\n","flags.DEFINE_integer(\r\n","    \"num_tpu_cores\", 8,\r\n","    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\r\n","\r\n","tf.flags.DEFINE_string(\r\n","    \"tpu_name\", None,\r\n","    \"The Cloud TPU to use for training. This should be either the name \"\r\n","    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\r\n","    \"url.\")\r\n","\r\n","tf.flags.DEFINE_string(\r\n","    \"tpu_zone\", None,\r\n","    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\r\n","    \"specified, we will attempt to automatically detect the GCE project from \"\r\n","    \"metadata.\")\r\n","\r\n","tf.flags.DEFINE_string(\r\n","    \"gcp_project\", None,\r\n","    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\r\n","    \"specified, we will attempt to automatically detect the GCE project from \"\r\n","    \"metadata.\")\r\n","\r\n","\r\n","class InputExample(object):\r\n","    \"\"\"A single training/test example for simple sequence classification.\"\"\"\r\n","\r\n","    def __init__(self, guid, text, label=None):\r\n","        \"\"\"Constructs a InputExample.\r\n","\r\n","        Args:\r\n","          guid: Unique id for the example.\r\n","          text_a: string. The untokenized text of the first sequence. For single\r\n","            sequence tasks, only this sequence must be specified.\r\n","          label: (Optional) string. The label of the example. This should be\r\n","            specified for train and dev examples, but not for test examples.\r\n","        \"\"\"\r\n","        self.guid = guid\r\n","        self.text = text\r\n","        self.label = label\r\n","\r\n","\r\n","class InputFeatures(object):\r\n","    \"\"\"A single set of features of data.\"\"\"\r\n","\r\n","    def __init__(self, input_ids, input_mask, segment_ids, label_ids, ):\r\n","        self.input_ids = input_ids\r\n","        self.input_mask = input_mask\r\n","        self.segment_ids = segment_ids\r\n","        self.label_ids = label_ids\r\n","        # self.label_mask = label_mask\r\n","\r\n","\r\n","class DataProcessor(object):\r\n","    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\r\n","\r\n","    def get_train_examples(self, data_dir):\r\n","        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\r\n","        raise NotImplementedError()\r\n","\r\n","    def get_dev_examples(self, data_dir):\r\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\r\n","        raise NotImplementedError()\r\n","\r\n","    def get_labels(self):\r\n","        \"\"\"Gets the list of labels for this data set.\"\"\"\r\n","        raise NotImplementedError()\r\n","\r\n","    @classmethod\r\n","    def _read_data(cls, input_file):\r\n","        \"\"\"Reads a BIO data.\"\"\"\r\n","        with tf.gfile.Open(input_file, \"r\") as f:\r\n","            lines = []\r\n","            words = []\r\n","            labels = []\r\n","            for line in f:\r\n","                contends = line.strip()\r\n","                if len(contends) == 0:\r\n","                    assert len(words) == len(labels)\r\n","                    if len(words) > 30:\r\n","                        # split if the sentence is longer than 30\r\n","                        while len(words) > 30:\r\n","                            tmplabel = labels[:30]\r\n","                            for iidx in range(len(tmplabel)):\r\n","                                if tmplabel.pop() == 'O':\r\n","                                    break\r\n","                            l = ' '.join(\r\n","                                [label for label in labels[:len(tmplabel) + 1] if len(label) > 0])\r\n","                            w = ' '.join(\r\n","                                [word for word in words[:len(tmplabel) + 1] if len(word) > 0])\r\n","                            lines.append([l, w])\r\n","                            words = words[len(tmplabel) + 1:]\r\n","                            labels = labels[len(tmplabel) + 1:]\r\n","\r\n","                    if len(words) == 0:\r\n","                        continue\r\n","                    l = ' '.join([label for label in labels if len(label) > 0])\r\n","                    w = ' '.join([word for word in words if len(word) > 0])\r\n","                    lines.append([l, w])\r\n","                    words = []\r\n","                    labels = []\r\n","                    continue\r\n","\r\n","                word = line.strip().split()[0]\r\n","                label = line.strip().split()[-1]\r\n","                words.append(word)\r\n","                labels.append(label)\r\n","            return lines\r\n","\r\n","\r\n","class BC5CDRProcessor(DataProcessor):\r\n","    def get_train_examples(self, data_dir):\r\n","        l1 = self._read_data(os.path.join(data_dir, \"train.tsv\"))\r\n","        l2 = self._read_data(os.path.join(data_dir, \"devel.tsv\"))\r\n","        return self._create_example(l1 + l2, \"train\")\r\n","\r\n","    def get_dev_examples(self, data_dir):\r\n","        return self._create_example(\r\n","            self._read_data(os.path.join(data_dir, \"devel.tsv\")), \"dev\"\r\n","        )\r\n","\r\n","    def get_test_examples(self, data_dir):\r\n","        return self._create_example(\r\n","            self._read_data(os.path.join(data_dir, \"test.tsv\")), \"test\")\r\n","\r\n","    def get_labels(self):\r\n","        return [\"B\", \"I\", \"O\", \"X\", \"[CLS]\", \"[SEP]\"]\r\n","\r\n","    def _create_example(self, lines, set_type):\r\n","        examples = []\r\n","        for (i, line) in enumerate(lines):\r\n","            guid = \"%s-%s\" % (set_type, i)\r\n","            text = tokenization.convert_to_unicode(line[1])\r\n","            label = tokenization.convert_to_unicode(line[0])\r\n","            examples.append(InputExample(guid=guid, text=text, label=label))\r\n","        return examples\r\n","\r\n","\r\n","class CLEFEProcessor(DataProcessor):\r\n","    def get_train_examples(self, data_dir):\r\n","        lines1 = self._read_data2(os.path.join(data_dir, \"Training.tsv\"))\r\n","        lines2 = self._read_data2(os.path.join(data_dir, \"Development.tsv\"))\r\n","        return self._create_example(\r\n","            lines1 + lines2, \"train\"\r\n","        )\r\n","\r\n","    def get_dev_examples(self, data_dir):\r\n","        return self._create_example(\r\n","            self._read_data2(os.path.join(data_dir, \"Development.tsv\")), \"dev\"\r\n","        )\r\n","\r\n","    def get_test_examples(self, data_dir):\r\n","        return self._create_example(\r\n","            self._read_data2(os.path.join(data_dir, \"Test.tsv\")), \"test\")\r\n","\r\n","    def get_labels(self):\r\n","        return [\"B\", \"I\", \"O\", \"X\", \"[CLS]\", \"[SEP]\"]\r\n","\r\n","    def _create_example(self, lines, set_type):\r\n","        examples = []\r\n","        for (i, line) in enumerate(lines):\r\n","            guid = \"%s-%s\" % (set_type, i)\r\n","            text = tokenization.convert_to_unicode(line[1])\r\n","            label = tokenization.convert_to_unicode(line[0])\r\n","            examples.append(InputExample(guid=guid, text=text, label=label))\r\n","        return examples\r\n","\r\n","    @classmethod\r\n","    def _read_data2(cls, input_file):\r\n","        with tf.gfile.Open(input_file, \"r\") as f:\r\n","            lines = []\r\n","            words = []\r\n","            labels = []\r\n","            for line in f:\r\n","                contends = line.strip()\r\n","                if len(contends) == 0:\r\n","                    assert len(words) == len(labels)\r\n","                    if len(words) == 0:\r\n","                        continue\r\n","                    l = ' '.join([label for label in labels if len(label) > 0])\r\n","                    w = ' '.join([word for word in words if len(word) > 0])\r\n","                    lines.append([l, w])\r\n","                    words = []\r\n","                    labels = []\r\n","                    continue\r\n","                elif contends.startswith('###'):\r\n","                    continue\r\n","\r\n","                word = line.strip().split()[0]\r\n","                label = line.strip().split()[-1]\r\n","                words.append(word)\r\n","                labels.append(label)\r\n","            return lines\r\n","\r\n","\r\n","def write_tokens(tokens, labels, mode):\r\n","    if mode == \"test\":\r\n","        path = os.path.join(FLAGS.output_dir, \"token_\" + mode + \".txt\")\r\n","        if tf.gfile.Exists(path):\r\n","            wf = tf.gfile.Open(path, 'a')\r\n","        else:\r\n","            wf = tf.gfile.Open(path, 'w')\r\n","        for token, label in zip(tokens, labels):\r\n","            if token != \"**NULL**\":\r\n","                wf.write(token + ' ' + str(label) + '\\n')\r\n","        wf.close()\r\n","\r\n","\r\n","def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, mode):\r\n","    label_map = {}\r\n","    for (i, label) in enumerate(label_list, 1):\r\n","        label_map[label] = i\r\n","    label2id_file = os.path.join(FLAGS.output_dir, 'label2id.pkl')\r\n","    if not tf.gfile.Exists(label2id_file):\r\n","        with tf.gfile.Open(label2id_file, 'wb') as w:\r\n","            pickle.dump(label_map, w)\r\n","    textlist = example.text.split(' ')\r\n","    labellist = example.label.split(' ')\r\n","    tokens = []\r\n","    labels = []\r\n","    for i, word in enumerate(textlist):\r\n","        token = tokenizer.tokenize(word)\r\n","        tokens.extend(token)\r\n","        label_1 = labellist[i]\r\n","        for m in range(len(token)):\r\n","            if m == 0:\r\n","                labels.append(label_1)\r\n","            else:\r\n","                labels.append(\"X\")\r\n","    # tokens = tokenizer.tokenize(example.text)\r\n","    if len(tokens) >= max_seq_length - 1:\r\n","        tokens = tokens[0:(max_seq_length - 2)]\r\n","        labels = labels[0:(max_seq_length - 2)]\r\n","    ntokens = []\r\n","    segment_ids = []\r\n","    label_ids = []\r\n","    ntokens.append(\"[CLS]\")\r\n","    segment_ids.append(0)\r\n","    # append(\"O\") or append(\"[CLS]\") not sure!\r\n","    label_ids.append(label_map[\"[CLS]\"])\r\n","    for i, token in enumerate(tokens):\r\n","        ntokens.append(token)\r\n","        segment_ids.append(0)\r\n","        label_ids.append(label_map[labels[i]])\r\n","    ntokens.append(\"[SEP]\")\r\n","    segment_ids.append(0)\r\n","    # append(\"O\") or append(\"[SEP]\") not sure!\r\n","    label_ids.append(label_map[\"[SEP]\"])\r\n","    input_ids = tokenizer.convert_tokens_to_ids(ntokens)\r\n","    input_mask = [1] * len(input_ids)\r\n","    # label_mask = [1] * len(input_ids)\r\n","    while len(input_ids) < max_seq_length:\r\n","        input_ids.append(0)\r\n","        input_mask.append(0)\r\n","        segment_ids.append(0)\r\n","        # we don't concerned about it!\r\n","        label_ids.append(0)\r\n","        ntokens.append(\"**NULL**\")\r\n","        # label_mask.append(0)\r\n","    # print(len(input_ids))\r\n","    assert len(input_ids) == max_seq_length\r\n","    assert len(input_mask) == max_seq_length\r\n","    assert len(segment_ids) == max_seq_length\r\n","    assert len(label_ids) == max_seq_length\r\n","    # assert len(label_mask) == max_seq_length\r\n","\r\n","    if ex_index < 5:\r\n","        tf.logging.info(\"*** Example ***\")\r\n","        tf.logging.info(\"guid: %s\" % (example.guid))\r\n","        tf.logging.info(\"tokens: %s\" % \" \".join(\r\n","            [tokenization.printable_text(x) for x in tokens]))\r\n","        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\r\n","        tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\r\n","        tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\r\n","        tf.logging.info(\"label_ids: %s\" % \" \".join([str(x) for x in label_ids]))\r\n","        # tf.logging.info(\"label_mask: %s\" % \" \".join([str(x) for x in label_mask]))\r\n","\r\n","    feature = InputFeatures(\r\n","        input_ids=input_ids,\r\n","        input_mask=input_mask,\r\n","        segment_ids=segment_ids,\r\n","        label_ids=label_ids,\r\n","        # label_mask = label_mask\r\n","    )\r\n","    # write_tokens(ntokens, label_ids, mode)\r\n","    return feature\r\n","\r\n","\r\n","def filed_based_convert_examples_to_features(\r\n","        examples, label_list, max_seq_length, tokenizer, output_file, mode=None):\r\n","    writer = tf.python_io.TFRecordWriter(output_file)\r\n","    for (ex_index, example) in enumerate(examples):\r\n","        if ex_index % 5000 == 0:\r\n","            tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\r\n","        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer,\r\n","                                         mode)\r\n","\r\n","        def create_int_feature(values):\r\n","            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\r\n","            return f\r\n","\r\n","        features = collections.OrderedDict()\r\n","        features[\"input_ids\"] = create_int_feature(feature.input_ids)\r\n","        features[\"input_mask\"] = create_int_feature(feature.input_mask)\r\n","        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\r\n","        features[\"label_ids\"] = create_int_feature(feature.label_ids)\r\n","        # features[\"label_mask\"] = create_int_feature(feature.label_mask)\r\n","        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\r\n","        writer.write(tf_example.SerializeToString())\r\n","\r\n","\r\n","def file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder):\r\n","    name_to_features = {\r\n","        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\r\n","        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\r\n","        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\r\n","        \"label_ids\": tf.FixedLenFeature([seq_length], tf.int64),\r\n","        # \"label_ids\":tf.VarLenFeature(tf.int64),\r\n","        # \"label_mask\": tf.FixedLenFeature([seq_length], tf.int64),\r\n","    }\r\n","\r\n","    def _decode_record(record, name_to_features):\r\n","        example = tf.parse_single_example(record, name_to_features)\r\n","        for name in list(example.keys()):\r\n","            t = example[name]\r\n","            if t.dtype == tf.int64:\r\n","                t = tf.to_int32(t)\r\n","            example[name] = t\r\n","        return example\r\n","\r\n","    def input_fn(params):\r\n","        batch_size = params[\"batch_size\"]\r\n","        d = tf.data.TFRecordDataset(input_file)\r\n","        if is_training:\r\n","            d = d.repeat()\r\n","            d = d.shuffle(buffer_size=100)\r\n","        d = d.apply(tf.contrib.data.map_and_batch(\r\n","            lambda record: _decode_record(record, name_to_features),\r\n","            batch_size=batch_size,\r\n","            drop_remainder=drop_remainder\r\n","        ))\r\n","        return d\r\n","\r\n","    return input_fn\r\n","\r\n","\r\n","def create_model(bert_config, is_training, input_ids, input_mask,\r\n","                 segment_ids, labels, num_labels, use_one_hot_embeddings):\r\n","    model = modeling.BertModel(\r\n","        config=bert_config,\r\n","        is_training=is_training,\r\n","        input_ids=input_ids,\r\n","        input_mask=input_mask,\r\n","        token_type_ids=segment_ids,\r\n","        use_one_hot_embeddings=use_one_hot_embeddings\r\n","    )\r\n","\r\n","    output_layer = model.get_sequence_output()\r\n","\r\n","    hidden_size = output_layer.shape[-1].value\r\n","\r\n","    output_weight = tf.get_variable(\r\n","        \"output_weights\", [num_labels, hidden_size],\r\n","        initializer=tf.truncated_normal_initializer(stddev=0.02)\r\n","    )\r\n","    output_bias = tf.get_variable(\r\n","        \"output_bias\", [num_labels], initializer=tf.zeros_initializer()\r\n","    )\r\n","    with tf.variable_scope(\"loss\"):\r\n","        if is_training:\r\n","            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\r\n","        output_layer = tf.reshape(output_layer, [-1, hidden_size])\r\n","        logits = tf.matmul(output_layer, output_weight, transpose_b=True)\r\n","        logits = tf.nn.bias_add(logits, output_bias)\r\n","        logits = tf.reshape(logits, [-1, FLAGS.max_seq_length, num_labels])\r\n","        # mask = tf.cast(input_mask,tf.float32)\r\n","        # loss = tf.contrib.seq2seq.sequence_loss(logits,labels,mask)\r\n","        # return (loss, logits, predict)\r\n","        ##########################################################################\r\n","        log_probs = tf.nn.log_softmax(logits, axis=-1)\r\n","        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\r\n","        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\r\n","        loss = tf.reduce_sum(per_example_loss)\r\n","        probabilities = tf.nn.softmax(logits, axis=-1)\r\n","        predict = tf.argmax(probabilities, axis=-1)\r\n","        return (loss, per_example_loss, logits, predict)\r\n","        ##########################################################################\r\n","\r\n","\r\n","def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\r\n","                     num_train_steps, num_warmup_steps, use_tpu,\r\n","                     use_one_hot_embeddings):\r\n","    def model_fn(features, labels, mode, params):\r\n","        tf.logging.info(\"*** Features ***\")\r\n","        for name in sorted(features.keys()):\r\n","            tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\r\n","        input_ids = features[\"input_ids\"]\r\n","        input_mask = features[\"input_mask\"]\r\n","        segment_ids = features[\"segment_ids\"]\r\n","        label_ids = features[\"label_ids\"]\r\n","        # label_mask = features[\"label_mask\"]\r\n","        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n","\r\n","        (total_loss, per_example_loss, logits, predicts) = create_model(\r\n","            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\r\n","            num_labels, use_one_hot_embeddings)\r\n","        tvars = tf.trainable_variables()\r\n","        scaffold_fn = None\r\n","        if init_checkpoint:\r\n","            (assignment_map,\r\n","             initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars,\r\n","                                                                                       init_checkpoint)\r\n","            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n","            if use_tpu:\r\n","                def tpu_scaffold():\r\n","                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n","                    return tf.train.Scaffold()\r\n","\r\n","                scaffold_fn = tpu_scaffold\r\n","            else:\r\n","                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n","        tf.logging.info(\"**** Trainable Variables ****\")\r\n","\r\n","        for var in tvars:\r\n","            init_string = \"\"\r\n","            if var.name in initialized_variable_names:\r\n","                init_string = \", *INIT_FROM_CKPT*\"\r\n","            tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\r\n","                            init_string)\r\n","        output_spec = None\r\n","        if mode == tf.estimator.ModeKeys.TRAIN:\r\n","            train_op = optimization.create_optimizer(\r\n","                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\r\n","            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n","                mode=mode,\r\n","                loss=total_loss,\r\n","                train_op=train_op,\r\n","                scaffold_fn=scaffold_fn)\r\n","        elif mode == tf.estimator.ModeKeys.EVAL:\r\n","\r\n","            def metric_fn(per_example_loss, label_ids, logits):\r\n","                # def metric_fn(label_ids, logits):\r\n","                predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\r\n","                precision = tf_metrics.precision(label_ids, predictions, num_labels, [1, 2], average=\"macro\")\r\n","                recall = tf_metrics.recall(label_ids, predictions, num_labels, [1, 2], average=\"macro\")\r\n","                f = tf_metrics.f1(label_ids, predictions, num_labels, [1, 2], average=\"macro\")\r\n","                #\r\n","                return {\r\n","                    \"eval_precision\": precision,\r\n","                    \"eval_recall\": recall,\r\n","                    \"eval_f\": f,\r\n","                    # \"eval_loss\": loss,\r\n","                }\r\n","\r\n","            eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\r\n","            # eval_metrics = (metric_fn, [label_ids, logits])\r\n","            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n","                mode=mode,\r\n","                loss=total_loss,\r\n","                eval_metrics=eval_metrics,\r\n","                scaffold_fn=scaffold_fn)\r\n","        else:\r\n","            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n","                mode=mode, predictions=predicts, scaffold_fn=scaffold_fn\r\n","            )\r\n","        return output_spec\r\n","\r\n","    return model_fn\r\n","\r\n","\r\n","def result_to_pair(predict_examples, predictions, id2label, output_predict_file,\r\n","                   output_err_file):\r\n","    \"\"\"\r\n","    Args:\r\n","        predict_examples (list): InputExample, no X\r\n","    \"\"\"\r\n","    if len(predict_examples) != len(predictions):\r\n","        tf.logging.error('{} vs {}'.format(len(predict_examples), len(predictions)))\r\n","    print(output_predict_file)\r\n","    print(output_err_file)\r\n","    with tf.gfile.Open(output_predict_file, 'w') as writer, \\\r\n","            tf.gfile.Open(output_err_file, 'w') as err_writer:\r\n","        for predict_line, pred_ids in zip(predict_examples, predictions):\r\n","            words = str(predict_line.text).split(' ')\r\n","            labels = str(predict_line.label).split(' ')\r\n","            if len(words) != len(labels):\r\n","                tf.logging.error('Text and label not equal')\r\n","                tf.logging.error(predict_line.text)\r\n","                tf.logging.error(predict_line.label)\r\n","                exit(1)\r\n","\r\n","            # get from CLS to SEP\r\n","            pred_labels = []\r\n","            for id in pred_ids:\r\n","                if id == 0:\r\n","                    continue\r\n","                curr_label = id2label[id]\r\n","                if curr_label == '[CLS]':\r\n","                    continue\r\n","                elif curr_label == '[SEP]':\r\n","                    break\r\n","                elif curr_label == 'X':\r\n","                    continue\r\n","                pred_labels.append(curr_label)\r\n","            if len(pred_labels) > len(words):\r\n","                # tf.logging.error(predict_line.text)\r\n","                # tf.logging.error(predict_line.label)\r\n","                # tf.logging.error(words)\r\n","                # tf.logging.error(labels)\r\n","                # tf.logging.error(pred_labels)\r\n","                err_writer.write(predict_line.guid + '\\n')\r\n","                err_writer.write(predict_line.text + '\\n')\r\n","                err_writer.write(predict_line.label + '\\n')\r\n","                err_writer.write(' '.join([str(i) for i in pred_ids]) + '\\n')\r\n","                err_writer.write(' '.join([id2label.get(i, '**NULL**') for i in pred_ids]) + '\\n\\n')\r\n","                pred_labels = pred_labels[:len(words)]\r\n","            elif len(pred_labels) < len(words):\r\n","                # tf.logging.error(predict_line.text)\r\n","                # tf.logging.error(predict_line.label)\r\n","                # tf.logging.error(words)\r\n","                # tf.logging.error(labels)\r\n","                # tf.logging.error(pred_labels)\r\n","                err_writer.write(predict_line.guid + '\\n')\r\n","                err_writer.write(predict_line.text + '\\n')\r\n","                err_writer.write(predict_line.label + '\\n')\r\n","                err_writer.write(' '.join([str(i) for i in pred_ids]) + '\\n')\r\n","                err_writer.write(' '.join([id2label.get(i, '**NULL**') for i in pred_ids]) + '\\n\\n')\r\n","                pred_labels += ['O'] * (len(words) - len(pred_labels))\r\n","\r\n","            for tok, label, pred_label in zip(words, labels, pred_labels):\r\n","                writer.write(tok + ' ' + label + ' ' + pred_label + '\\n')\r\n","            writer.write('\\n')\r\n","\r\n","\r\n","def main(_):\r\n","    tf.logging.set_verbosity(tf.logging.INFO)\r\n","    processors = {\r\n","        \"bc5cdr\": BC5CDRProcessor,\r\n","        \"clefe\": CLEFEProcessor,\r\n","    }\r\n","    # if not FLAGS.do_train and not FLAGS.do_eval:\r\n","    #    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\r\n","\r\n","    bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\r\n","\r\n","    if FLAGS.max_seq_length > bert_config.max_position_embeddings:\r\n","        raise ValueError(\r\n","            \"Cannot use sequence length %d because the BERT model \"\r\n","            \"was only trained up to sequence length %d\" %\r\n","            (FLAGS.max_seq_length, bert_config.max_position_embeddings))\r\n","\r\n","    task_name = FLAGS.task_name.lower()\r\n","    if task_name not in processors:\r\n","        raise ValueError(\"Task not found: %s\" % (task_name))\r\n","\r\n","    tf.gfile.MakeDirs(FLAGS.output_dir)\r\n","\r\n","    processor = processors[task_name]()\r\n","\r\n","    label_list = processor.get_labels()\r\n","\r\n","    tokenizer = tokenization.FullTokenizer(\r\n","        vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\r\n","    tpu_cluster_resolver = None\r\n","    if FLAGS.use_tpu and FLAGS.tpu_name:\r\n","        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\r\n","            FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\r\n","\r\n","    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\r\n","\r\n","    run_config = tf.contrib.tpu.RunConfig(\r\n","        cluster=tpu_cluster_resolver,\r\n","        master=FLAGS.master,\r\n","        model_dir=FLAGS.output_dir,\r\n","        save_checkpoints_steps=FLAGS.save_checkpoints_steps,\r\n","        tpu_config=tf.contrib.tpu.TPUConfig(\r\n","            iterations_per_loop=FLAGS.iterations_per_loop,\r\n","            num_shards=FLAGS.num_tpu_cores,\r\n","            per_host_input_for_training=is_per_host))\r\n","\r\n","    train_examples = None\r\n","    num_train_steps = None\r\n","    num_warmup_steps = None\r\n","\r\n","    if FLAGS.do_train:\r\n","        train_examples = processor.get_train_examples(FLAGS.data_dir)\r\n","        num_train_steps = int(\r\n","            len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\r\n","        num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\r\n","\r\n","    model_fn = model_fn_builder(\r\n","        bert_config=bert_config,\r\n","        num_labels=len(label_list) + 1,\r\n","        init_checkpoint=FLAGS.init_checkpoint,\r\n","        learning_rate=FLAGS.learning_rate,\r\n","        num_train_steps=num_train_steps,\r\n","        num_warmup_steps=num_warmup_steps,\r\n","        use_tpu=FLAGS.use_tpu,\r\n","        use_one_hot_embeddings=FLAGS.use_tpu)\r\n","\r\n","    estimator = tf.contrib.tpu.TPUEstimator(\r\n","        use_tpu=FLAGS.use_tpu,\r\n","        model_fn=model_fn,\r\n","        config=run_config,\r\n","        train_batch_size=FLAGS.train_batch_size,\r\n","        eval_batch_size=FLAGS.eval_batch_size,\r\n","        predict_batch_size=FLAGS.predict_batch_size)\r\n","\r\n","    if FLAGS.do_train:\r\n","        train_file = os.path.join(FLAGS.output_dir, \"train.tf_record\")\r\n","        filed_based_convert_examples_to_features(\r\n","            train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\r\n","        tf.logging.info(\"***** Running training *****\")\r\n","        tf.logging.info(\"  Num examples = %d\", len(train_examples))\r\n","        tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\r\n","        tf.logging.info(\"  Num steps = %d\", num_train_steps)\r\n","        train_input_fn = file_based_input_fn_builder(\r\n","            input_file=train_file,\r\n","            seq_length=FLAGS.max_seq_length,\r\n","            is_training=True,\r\n","            drop_remainder=True)\r\n","        estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n","    if FLAGS.do_eval:\r\n","        eval_examples = processor.get_dev_examples(FLAGS.data_dir)\r\n","        eval_file = os.path.join(FLAGS.output_dir, \"eval.tf_record\")\r\n","        filed_based_convert_examples_to_features(\r\n","            eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\r\n","\r\n","        tf.logging.info(\"***** Running evaluation *****\")\r\n","        tf.logging.info(\"  Num examples = %d\", len(eval_examples))\r\n","        tf.logging.info(\"  Batch size = %d\", FLAGS.eval_batch_size)\r\n","        eval_steps = None\r\n","        if FLAGS.use_tpu:\r\n","            eval_steps = int(len(eval_examples) / FLAGS.eval_batch_size)\r\n","        eval_drop_remainder = True if FLAGS.use_tpu else False\r\n","        eval_input_fn = file_based_input_fn_builder(\r\n","            input_file=eval_file,\r\n","            seq_length=FLAGS.max_seq_length,\r\n","            is_training=False,\r\n","            drop_remainder=eval_drop_remainder)\r\n","        result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\r\n","        output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results.txt\")\r\n","        with tf.gfile.Open(output_eval_file, \"w\") as writer:\r\n","            tf.logging.info(\"***** Eval results *****\")\r\n","            for key in sorted(result.keys()):\r\n","                tf.logging.info(\"  %s = %s\", key, str(result[key]))\r\n","                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\r\n","    if FLAGS.do_predict:\r\n","        with tf.gfile.Open(os.path.join(FLAGS.output_dir, 'label2id.pkl'), 'rb') as rf:\r\n","            label2id = pickle.load(rf)\r\n","            id2label = {value: key for key, value in label2id.items()}\r\n","        token_path = os.path.join(FLAGS.output_dir, \"token_test.txt\")\r\n","        if tf.gfile.Exists(token_path):\r\n","            tf.gfile.Remove(token_path)\r\n","        predict_examples = processor.get_test_examples(FLAGS.data_dir)\r\n","\r\n","        predict_file = os.path.join(FLAGS.output_dir, \"predict.tf_record\")\r\n","        filed_based_convert_examples_to_features(predict_examples, label_list,\r\n","                                                 FLAGS.max_seq_length, tokenizer,\r\n","                                                 predict_file, mode=\"test\")\r\n","\r\n","        tf.logging.info(\"***** Running prediction*****\")\r\n","        tf.logging.info(\"  Num examples = %d\", len(predict_examples))\r\n","        tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\r\n","        if FLAGS.use_tpu:\r\n","            # Warning: According to tpu_estimator.py Prediction on TPU is an\r\n","            # experimental feature and hence not supported here\r\n","            raise ValueError(\"Prediction in TPU not supported\")\r\n","        predict_drop_remainder = True if FLAGS.use_tpu else False\r\n","        predict_input_fn = file_based_input_fn_builder(\r\n","            input_file=predict_file,\r\n","            seq_length=FLAGS.max_seq_length,\r\n","            is_training=False,\r\n","            drop_remainder=predict_drop_remainder)\r\n","\r\n","        prf = estimator.evaluate(input_fn=predict_input_fn, steps=None)\r\n","        output_test_file = os.path.join(FLAGS.output_dir, \"test_results.txt\")\r\n","        with tf.gfile.Open(output_test_file, \"w\") as writer:\r\n","            tf.logging.info(\"***** TEST results *****\")\r\n","            for key in sorted(prf.keys()):\r\n","                tf.logging.info(\"  %s = %s\", key, str(prf[key]))\r\n","                writer.write(\"%s = %s\\n\" % (key, str(prf[key])))\r\n","\r\n","        result = estimator.predict(input_fn=predict_input_fn)\r\n","        result = list(result)\r\n","        output_predict_file = os.path.join(FLAGS.output_dir, \"label_test.txt\")\r\n","        with tf.gfile.Open(output_predict_file, 'w') as writer:\r\n","            print(id2label)\r\n","            for prediction in result:\r\n","                output_line = \"\\n\".join(id2label[id] for id in prediction if id != 0) + \"\\n\"\r\n","                writer.write(output_line)\r\n","\r\n","        output_predict_file = os.path.join(FLAGS.output_dir, \"test_labels.txt\")\r\n","        output_err_file = os.path.join(FLAGS.output_dir, \"test_labels_errs.txt\")\r\n","        result_to_pair(predict_examples, result, id2label,\r\n","                       output_predict_file, output_err_file)\r\n","\r\n","        tf.logging.info('Reading: %s', output_predict_file)\r\n","        with tf.gfile.Open(output_predict_file, \"r\") as f:\r\n","            counts = evaluate(f)\r\n","        eval_result = report_notprint(counts)\r\n","        print(''.join(eval_result))\r\n","        with tf.gfile.Open(os.path.join(FLAGS.output_dir, 'test_results_conlleval.txt'), 'w') as fd:\r\n","            fd.write(''.join(eval_result))\r\n","\r\n","\r\n","if __name__ == \"__main__\":\r\n","    flags.mark_flag_as_required(\"data_dir\")\r\n","    flags.mark_flag_as_required(\"task_name\")\r\n","    flags.mark_flag_as_required(\"vocab_file\")\r\n","    flags.mark_flag_as_required(\"bert_config_file\")\r\n","    flags.mark_flag_as_required(\"output_dir\")\r\n","    tf.app.run()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /content/drive/MyDrive/Colab Notebooks/Capstone/bluebert/bert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/absl/flags/_validators.py:359: UserWarning: Flag --data_dir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!\n","  'command line!' % flag_name)\n","/usr/local/lib/python3.7/dist-packages/absl/flags/_validators.py:359: UserWarning: Flag --task_name has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!\n","  'command line!' % flag_name)\n","/usr/local/lib/python3.7/dist-packages/absl/flags/_validators.py:359: UserWarning: Flag --vocab_file has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!\n","  'command line!' % flag_name)\n","/usr/local/lib/python3.7/dist-packages/absl/flags/_validators.py:359: UserWarning: Flag --bert_config_file has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!\n","  'command line!' % flag_name)\n","/usr/local/lib/python3.7/dist-packages/absl/flags/_validators.py:359: UserWarning: Flag --output_dir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!\n","  'command line!' % flag_name)\n","W0301 15:36:01.943361 140623561652096 module_wrapper.py:139] From /content/drive/MyDrive/Colab Notebooks/Capstone/bluebert/bert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["W0301 15:36:02.798427 140623561652096 lazy_loader.py:50] \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fe52a48ef80>) includes params argument, but params are not passed to Estimator.\n"],"name":"stdout"},{"output_type":"stream","text":["W0301 15:36:04.704784 140623561652096 estimator.py:1994] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fe52a48ef80>) includes params argument, but params are not passed to Estimator.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'output/BC5CDRdiseas_10/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe52a163c10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n"],"name":"stdout"},{"output_type":"stream","text":["I0301 15:36:04.708392 140623561652096 estimator.py:212] Using config: {'_model_dir': 'output/BC5CDRdiseas_10/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe52a163c10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:_TPUContext: eval_on_tpu True\n"],"name":"stdout"},{"output_type":"stream","text":["I0301 15:36:04.710912 140623561652096 tpu_context.py:220] _TPUContext: eval_on_tpu True\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n"],"name":"stdout"},{"output_type":"stream","text":["W0301 15:36:04.712823 140623561652096 tpu_context.py:222] eval_on_tpu ignored because use_tpu is False.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 13122\n"],"name":"stdout"},{"output_type":"stream","text":["I0301 15:36:04.717437 140623561652096 <ipython-input-7-e7e67ac6ddbc>:386] Writing example 0 of 13122\n"],"name":"stderr"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-e7e67ac6ddbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_flag_as_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert_config_file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_flag_as_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_dir\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-e7e67ac6ddbc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train.tf_record\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         filed_based_convert_examples_to_features(\n\u001b[0;32m--> 703\u001b[0;31m             train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"***** Running training *****\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  Num examples = %d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-e7e67ac6ddbc>\u001b[0m in \u001b[0;36mfiled_based_convert_examples_to_features\u001b[0;34m(examples, label_list, max_seq_length, tokenizer, output_file, mode)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Writing example %d of %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mex_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer,\n\u001b[0;32m--> 388\u001b[0;31m                                          mode)\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcreate_int_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-e7e67ac6ddbc>\u001b[0m in \u001b[0;36mconvert_single_example\u001b[0;34m(ex_index, example, label_list, max_seq_length, tokenizer, mode)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel2id_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel2id_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0mtextlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0mlabellist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"]}]},{"cell_type":"code","metadata":{"id":"BXkJAQR1bXMt"},"source":[""],"execution_count":null,"outputs":[]}]}